{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing information need 1...\n",
      "Processing system Base System...\n",
      "Processing system Enhanced System...\n",
      "Processing system New Schema System...\n",
      "Processing system Synonyms System...\n",
      "Processing system Keywords System...\n",
      "Processing system Embeddings System...\n",
      "Processing system Combined System...\n",
      "Processing information need 2...\n",
      "Processing system Base System...\n",
      "Processing system Enhanced System...\n",
      "Processing system New Schema System...\n",
      "Processing system Synonyms System...\n",
      "Processing system Keywords System...\n",
      "Processing system Embeddings System...\n",
      "Processing system Combined System...\n",
      "Processing information need 3...\n",
      "Processing system Base System...\n",
      "Processing system Enhanced System...\n",
      "Processing system New Schema System...\n",
      "Processing system Synonyms System...\n",
      "Processing system Keywords System...\n",
      "Processing system Embeddings System...\n",
      "Processing system Combined System...\n",
      "Processing information need 4...\n",
      "Processing system Base System...\n",
      "Processing system Enhanced System...\n",
      "Processing system New Schema System...\n",
      "Processing system Synonyms System...\n",
      "Processing system Keywords System...\n",
      "Processing system Embeddings System...\n",
      "Processing system Combined System...\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import PrecisionRecallDisplay\n",
    "import numpy as np\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Helper function to sanitize query names for filenames\n",
    "def sanitize_query(query):\n",
    "    return query.replace(' ', '_')\n",
    "\n",
    "# Read configuration from JSON file\n",
    "with open('../queries.json', 'r') as config_file:\n",
    "    config = json.load(config_file)\n",
    "\n",
    "# METRICS TABLE\n",
    "metrics = {}\n",
    "metric = lambda f: metrics.setdefault(f.__name__, f)\n",
    "\n",
    "@metric\n",
    "def ap(results, relevant):\n",
    "    precision_values = []\n",
    "    relevant_count = 0\n",
    "\n",
    "    for idx, doc in enumerate(results):\n",
    "        if doc['book_id'] in relevant:\n",
    "            relevant_count += 1\n",
    "            precision_at_k = relevant_count / (idx + 1)\n",
    "            precision_values.append(precision_at_k)\n",
    "\n",
    "    if not precision_values:\n",
    "        return 0.0\n",
    "\n",
    "    return sum(precision_values)/len(precision_values)\n",
    "\n",
    "@metric\n",
    "def p10(results, relevant, n=10):\n",
    "    return len([doc for doc in results[:n] if doc['book_id'] in relevant])/n\n",
    "\n",
    "def calculate_metric(key, results, relevant):\n",
    "    return metrics[key](results, relevant)\n",
    "\n",
    "evaluation_metrics = {\n",
    "    'ap': 'Average Precision',\n",
    "    'p10': 'Precision at 10 (P@10)'\n",
    "}\n",
    "\n",
    "# Container for results data\n",
    "all_results_data = []\n",
    "\n",
    "# Counter for assigning IDs\n",
    "id_counter = 1\n",
    "\n",
    "# create a mapping for the system names with _ and the pretty names\n",
    "system_names = [\n",
    "    \n",
    "    \"Base System\",\n",
    "    \"Enhanced System\",\n",
    "    \"New Schema System\",\n",
    "    \"Synonyms System\",\n",
    "    \"Keywords System\",\n",
    "    \"Embeddings System\",\n",
    "    \"Combined System\",\n",
    "\n",
    "]\n",
    "\n",
    "# create a mapping for the systems names and the average precision sum\n",
    "average_precision_sum = {\n",
    "    \"Base System\": 0,\n",
    "    \"Enhanced System\": 0,\n",
    "    \"New Schema System\": 0,\n",
    "    \"Synonyms System\": 0,\n",
    "    \"Keywords System\": 0,\n",
    "    \"Embeddings System\": 0,\n",
    "    \"Combined System\": 0,\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for info_need in config['information_needs']:\n",
    "    print(f'Processing information need {id_counter}...')\n",
    "    query = info_need['query']\n",
    "    sanitized_query = sanitize_query(query)\n",
    "    qrels_file = info_need['qrels_file']\n",
    "    folder_name = f'IN_{id_counter}/results'\n",
    "\n",
    "    # Create a directory for results\n",
    "    if not os.path.exists(folder_name):\n",
    "        os.makedirs(folder_name)\n",
    "    else:\n",
    "        # delete all files in the directory\n",
    "        files = os.listdir(folder_name)\n",
    "        for file in files:\n",
    "            os.remove(os.path.join(folder_name, file))\n",
    "        \n",
    "\n",
    "    relevant = list(map(lambda el: int(el.strip()), open(qrels_file).readlines()))\n",
    "\n",
    "    results_data = []\n",
    "\n",
    "    # create a data frame for the relevance with columns rank from 1 to 40 and a column for each system\n",
    "    relevance_df = pd.DataFrame(columns=['Rank'])\n",
    "    relevance_df['Rank'] = range(1, 41)\n",
    "    \n",
    "\n",
    "    for system in info_need['systems']:\n",
    "        \n",
    "\n",
    "        if system['name'] in system_names:\n",
    "            print(f'Processing system {system[\"name\"]}...')\n",
    "            system_name = system['name']\n",
    "            query_url = system['query_url']\n",
    "            \n",
    "            if system_name == \"Embeddings System\":\n",
    "                # load results from json file\n",
    "                with open(f'../embeddings/results/IN_{id_counter}.json', 'r') as f:\n",
    "                    results = json.load(f)['response']['docs']\n",
    "            else:\n",
    "                results = requests.get(query_url).json()['response']['docs']\n",
    "\n",
    "\n",
    "            # Calculate metrics and export results\n",
    "            df = pd.DataFrame([['Metric', 'Value']] +\n",
    "                            [[evaluation_metrics[m], calculate_metric(m, results, relevant)] for m in evaluation_metrics])\n",
    "\n",
    "            # export dataframe to latex\n",
    "            with open(f'{folder_name}/{system_name}.tex', 'w') as tf:\n",
    "                tf.write(df.to_latex())\n",
    "            with open(f'{folder_name}/{system_name}.csv', 'w') as tf:\n",
    "                tf.write(df.to_csv())\n",
    "\n",
    "            results_data.append((id_counter, system_name, results))\n",
    "\n",
    "            # create a dataframe with columns rank and relevance current system\n",
    "            df = pd.DataFrame(columns=['Rank', system_name])\n",
    "            df['Rank'] = range(1, 41)\n",
    "\n",
    "            # calculate the relevance for each document\n",
    "            for idx, doc in enumerate(results):\n",
    "                if doc['book_id'] in relevant:\n",
    "                    df.loc[idx, system_name] = \"R\"\n",
    "                else:\n",
    "                    df.loc[idx, system_name] = \"NR\"\n",
    "            \n",
    "\n",
    "            # add the relevance column to the relevance dataframe\n",
    "            relevance_df = pd.merge(relevance_df, df, on='Rank', how='outer')\n",
    "\n",
    "    # export relevance dataframe to latex\n",
    "    relevance_df.to_latex(f'{folder_name}/IN_{id_counter}_relevance.tex', index=False, header=True)\n",
    "\n",
    "    all_results_data.append((id_counter, query, results_data))\n",
    "    \n",
    "\n",
    "    # Combine results for precision-recall curve comparison\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    for _, system_name, results in results_data:\n",
    "        precision_values = [len([doc for doc in results[:idx+1] if doc['book_id'] in relevant]) / (idx+1) for idx, _ in enumerate(results)]\n",
    "        recall_values = [len([doc for doc in results[:idx+1] if doc['book_id'] in relevant]) / len(relevant) for idx, _ in enumerate(results)]\n",
    "\n",
    "        decreasing_max_precision = np.maximum.accumulate(precision_values[::-1])[::-1]\n",
    "        precision_recall_match = list(zip(recall_values, decreasing_max_precision))\n",
    "\n",
    "        if recall_values[-1] != 1:\n",
    "            precision_recall_match.append((1, precision_values[-1]))\n",
    "\n",
    "        plt.plot(*zip(*precision_recall_match), label=f'{system_name}')\n",
    "\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title(f'Interpolated Precision-Recall Curve Comparison')\n",
    "    plt.legend()\n",
    "    plt.savefig(f'{folder_name}/combined_IN_{id_counter}_precision_recall_comparison.png')\n",
    "    # plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    # Compare metrics in a table\n",
    "    merged_dfs = []\n",
    "\n",
    "    for _, system_name, _ in results_data:\n",
    "        system_df = pd.read_csv(f'{folder_name}/{system_name}.csv', index_col=0)\n",
    "        system_df = system_df.T\n",
    "        system_df.columns = system_df.iloc[0]\n",
    "        system_df = system_df.drop(system_df.index[0])\n",
    "        merged_dfs.append(system_df)\n",
    "\n",
    "    comparison_df = pd.concat(merged_dfs)\n",
    "    comparison_df['System'] = [f'{system_name}' for _, system_name, _ in results_data]\n",
    "    comparison_df = comparison_df[['System', 'Average Precision', 'Precision at 10 (P@10)']]\n",
    "    comparison_df['Average Precision'] = comparison_df['Average Precision'].astype(float)\n",
    "    comparison_df['Precision at 10 (P@10)'] = comparison_df['Precision at 10 (P@10)'].astype(float)\n",
    "\n",
    "    # add the average precision of each system to the same system in the average precision sum\n",
    "    for index, row in comparison_df.iterrows():\n",
    "        if row['System'] in average_precision_sum:\n",
    "            average_precision_sum[row['System']] += row['Average Precision']\n",
    "\n",
    "    # Print and export the comparison table\n",
    "    # print(comparison_df)\n",
    "    with open(f'{folder_name}/IN_{id_counter}_comparison.tex', 'w') as tf:\n",
    "        tf.write(comparison_df.to_latex(index=False, escape=False, float_format=\"%.6f\"))\n",
    "\n",
    "    id_counter += 1\n",
    "# Store all_results_data for later use if needed\n",
    "\n",
    "# calculate the average precision for each system\n",
    "for system_name in average_precision_sum:\n",
    "    average_precision_sum[system_name] /= 4\n",
    "\n",
    "# create a dataframe with the average precision sum\n",
    "average_precision_sum_df = pd.DataFrame.from_dict(average_precision_sum, orient='index', columns=['Average Precision'])\n",
    "average_precision_sum_df.index.name = 'System'\n",
    "# export the average precision sum dataframe to latex\n",
    "average_precision_sum_df.to_latex(f'average_precision.tex', escape=False, float_format=\"%.6f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
